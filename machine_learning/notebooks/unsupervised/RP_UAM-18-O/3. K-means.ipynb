{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Qm0x6PC1CZU"
   },
   "source": [
    "# 3. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLVwyKg50ak4"
   },
   "outputs": [],
   "source": [
    "# Carga bibliotecas\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlP9o3YL3bdZ"
   },
   "outputs": [],
   "source": [
    "# Genera N datos\n",
    "N = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1248,
     "status": "ok",
     "timestamp": 1537211966683,
     "user": {
      "displayName": "Oscar Yáñez",
      "photoUrl": "//lh5.googleusercontent.com/-Prxq0ut7kXg/AAAAAAAAAAI/AAAAAAAAAUE/0_0fIeQegvk/s50-c-k-no/photo.jpg",
      "userId": "101681087597011654825"
     },
     "user_tz": 300
    },
    "id": "2wgYkhkM7cJ2",
    "outputId": "fcea5e7d-e07b-48b9-87f0-4ce157595ee2"
   },
   "outputs": [],
   "source": [
    "# Blobs de distinta varianza\n",
    "Xblobs, Lblobs = datasets.make_blobs(n_samples=N, cluster_std=[1.0, 2.5, 0.5],\n",
    "                                       random_state=170)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada una distribución de datos no etiquetados, el método *k-means* estima iterativamente los *k* vectores de medias que mejor representen a los datos, utilizando para ello, el criterio de la distancia euclidiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: solo se grafican los vectores con etiquetas Lblobs=0\n",
    " @X_m: media de X\n",
    "\"\"\"\n",
    "X1   = Xblobs[Lblobs==0]\n",
    "X2   = Xblobs[Lblobs==1]\n",
    "X1_m = X1.mean(axis=0)\n",
    "X2_m = X2.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.plot(X1[:,0],X1[:,1],'ok',markersize=10,alpha=.8)\n",
    "\n",
    "plt.plot(X2[:,0],X2[:,1],'ok',markersize=10,alpha=.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX    = np.concatenate( [X1, X2] )\n",
    "\n",
    "model = kmeans(2)\n",
    "model.fit(XX)\n",
    "\n",
    "centers = model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los centroides:\n",
    " 1. Se inicializan aleatoriamente k vectores candidatos a centroides $\\mu_k\\in\\mathbb{R}^n$\n",
    " 2. Actualizar $\\mu_k$: $\\mu_k^{(i+1)}=\\mu_k^{(i)}+\\Delta f(\\mu_k^{(i)})$\n",
    " 3. Repetir 2 hasta convergencia, misma que siempre está garantizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note   : Muestra los centroides reales (verde) respecto a los encontrados con el método k-means (rojo)\n",
    " @XX     : Es la concatenación de los vectores de X1 y X2 para que sean vistos como un único conjunto de datos\n",
    " @centers: Centroides estimados por el método k-means\n",
    "\"\"\"\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.plot(XX[:,0],XX[:,1],'ok',markersize=10,alpha=.8)\n",
    "for x in centers:\n",
    "    plt.plot(x[0],x[1],'or',markersize=15,alpha=.8)\n",
    "\n",
    "plt.plot(X1_m[0],X1_m[1],'og',markersize=10,alpha=.8)\n",
    "plt.plot(X2_m[0],X2_m[1],'og',markersize=10,alpha=.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar la pertenencia se calcula la distancia euclidiana $e_{i,j}$ entre los vectores $x_i$ y los centroides $\\mu_j$ encontrados por el método k-means. El vector $x_i$ pertenece al cluster $C\\in k$\n",
    "$$C(x_i) = argmin\\left\\{ e_{i,j} \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: la función predict realiza el proceso anteriormente descrito\n",
    "\"\"\"\n",
    "LL = model.predict(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: grafica los clusters estimados por el método k-means\n",
    "\"\"\"\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.plot(XX[LL==0,0],XX[LL==0,1],'og',markersize=10,alpha=.8)\n",
    "plt.plot(XX[LL==1,0],XX[LL==1,1],'or',markersize=10,alpha=.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: grafica los clusters reales y los estimados.\n",
    " Donde existen puntos de dos colores significa que se han agrupado correctamente\n",
    " @note: dado que la inicialización de los centroides es aleatoria, hay que comentar/descomentar\n",
    " X1, X2 de forma adecuada para que se muestre correctamente la comparación\n",
    "\"\"\"\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.plot(XX[LL==0,0],XX[LL==0,1],'ok',markersize=10,alpha=.8)\n",
    "#plt.plot(X1[:    ,0],X1[    :,1],'or',markersize=5,alpha=.8)\n",
    "plt.plot(X2[:    ,0],X2[    :,1],'or',markersize=5,alpha=.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.plot(XX[LL==1,0],XX[LL==1,1],'ok',markersize=10,alpha=.8)\n",
    "plt.plot(X1[:    ,0],X1[    :,1],'or',markersize=5,alpha=.8)\n",
    "#plt.plot(X2[:    ,0],X2[    :,1],'or',markersize=5,alpha=.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios para Colaboratory\n",
    "\n",
    "Utilice k-means para estimar los clusters para los siguientes conjuntos de datos:\n",
    " - Lunas\n",
    " - Circulos concéntricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea para la Raspberry Pi\n",
    " 1. Utilice k-means para encontrar de forma no supervisada los tres conglomerados correspondientes a los tres tipos de Iris. Como sugerencia, utilice únicamente la proyección $\\mathbb{R}^2$ que mejor separe los datos (base su decisión en los *scatter plots*). Evalúe el desempeño del *clustering*\n",
    " 2. Utilice k-means para encontrar los conglomerados de datos que agrupen los vectores que representen a personas que están de pie y personas que están sentadas. Evalúe su desempeño."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "conjuntos_de_prueba_2d.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
