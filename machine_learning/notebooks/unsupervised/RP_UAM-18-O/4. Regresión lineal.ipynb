{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Qm0x6PC1CZU"
   },
   "source": [
    "# 4. Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLVwyKg50ak4"
   },
   "outputs": [],
   "source": [
    "# Carga bibliotecas útiles\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genera conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlP9o3YL3bdZ"
   },
   "outputs": [],
   "source": [
    "N = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1248,
     "status": "ok",
     "timestamp": 1537211966683,
     "user": {
      "displayName": "Oscar Yáñez",
      "photoUrl": "//lh5.googleusercontent.com/-Prxq0ut7kXg/AAAAAAAAAAI/AAAAAAAAAUE/0_0fIeQegvk/s50-c-k-no/photo.jpg",
      "userId": "101681087597011654825"
     },
     "user_tz": 300
    },
    "id": "2wgYkhkM7cJ2",
    "outputId": "fcea5e7d-e07b-48b9-87f0-4ce157595ee2"
   },
   "outputs": [],
   "source": [
    "# Blobs aniisotrópicos\n",
    "# Covarianza con dirección preferencial\n",
    "X, Lblobsa = datasets.make_blobs(n_samples=N, random_state=170)\n",
    "Xblobsa = np.dot(X, [[0.6, -0.6], [-0.4, 0.8]])\n",
    "\n",
    "# Blobs de distinta varianza\n",
    "Xblobsv, Lblobsv = datasets.make_blobs(n_samples=N, cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=170)\n",
    "plt.figure(figsize=(15,5))\n",
    "for X, l, i in zip((Xblobsa,Xblobsv),(Lblobsa,Lblobsv),range(1,4)):\n",
    "  plt.subplot(1,3,i)\n",
    "  plt.grid(True)\n",
    "  plt.scatter(X[l==0, 0], X[l==0, 1], s=20, color='red');\n",
    "  plt.scatter(X[l==1, 0], X[l==1, 1], s=20, color='blue');\n",
    "  plt.scatter(X[l==2, 0], X[l==2, 1], s=20, color='green');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión lineal es un método que permite estimar los hiperparámetros del hiperplano que mejor describa un conjunto de datos.\n",
    "\n",
    "Sea $X=\\{x_1, x_2,..., x_m\\}: x_i\\in\\mathbb{R}^n, \\forall i,m,n\\in\\mathbb{N}$ el conjunto de vectores independientes obtenidos por medio de una medición, mientras que $Y=\\{y_1, y_2,..., y_m\\}:\\in\\mathbb{R}$ corresponden a las variables medidas dependientes de $X$.\n",
    "\n",
    "Dada la ecuación general de un hiperplano\n",
    "$$y(x)=w^T x+b: w,x\\in\\mathbb{R}^n, y,b\\in\\mathbb{R},$$\n",
    "\n",
    "donde $w$ y $b$ son estimados a partir de $X$ y $Y$ por medio de un proceso de optimización y corresponden al vector normal al hiperplano estimado y al sesgo, respectivamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Únicamente se toman en cuenta uno de los blobs isotrópicos\n",
    " en este casos se muestra un conjunto de datos y como se vería una\n",
    " mala descripción de sus datos aproximados con y(x) = x\n",
    "\"\"\"\n",
    "x, y = Xblobsa[Lblobsa==0, 0], Xblobsa[Lblobsa==0, 1]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x,y,'o',markersize=5,color='orange')\n",
    "plt.plot(x,x,'-',color='black')\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Al medir las distancias de x_i con respecto al hiperplano y acumularlas\n",
    " se obtiene el error cuadrático\n",
    "\"\"\"\n",
    "x, y = Xblobsa[Lblobsa==0, 0], Xblobsa[Lblobsa==0, 1]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x,y,'o',markersize=5,color='orange')\n",
    "plt.plot(x,x,'-',color='black')\n",
    "\n",
    "for i,j in zip(x,y):\n",
    "    plt.arrow( i,j,0,i-j )\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = x\n",
    "ec    = np.dot( (y-y_est).T, (y-y_est) )\n",
    "\n",
    "print (ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión lineal basada en LDA se fundamenta en minimizar el error cuadrático medio acumulado de las distancias $x_i$ al hiperplano. La fundamentación teórica queda fuera de los alcances de este curso, es por ello que se utilizarán las bibliotecas de scikit-learn que realizan dicho proceso de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar bibliotecas para regresión\n",
    "\n",
    "from sklearn              import linear_model as LRM\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.svm          import SVR\n",
    "from sklearn.metrics      import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear objeto de regresión lineal y entrenarlo\n",
    "\n",
    "model = LRM.LinearRegression()\n",
    "#x[:,np.newaxis] se requiere porque se necesita un vector columna para el entrenamiento\n",
    "#si x.shape es de (m,n) para n>1 -> x[:,np.newaxis] no es necesario\n",
    "model.fit( x[:,np.newaxis],y) \n",
    "\n",
    "#en este caso se entrena y se predice con los mismos datos\n",
    "#únicamente con fines ilustrativos\n",
    "y_est = model.predict( x[:,np.newaxis] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec = np.dot( (y-y_est).T, (y-y_est) )\n",
    "\n",
    "#se observa que el error cuadrático se reduce drásticamente\n",
    "print (ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x,y,'o',markersize=5,color='orange')\n",
    "plt.plot(x,y_est,'-',color='black')\n",
    "\n",
    "for i,j,k in zip(x,y,y_est):\n",
    "    plt.arrow( i,j,0,k-j )\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Se utilizan métricas más robustas\n",
    " @mse: Error cuadrático medio\n",
    " @r2: Coeficiente de correlación de Pearson, miestras más cercano a -1 o 1 significa \n",
    " una anticorrelación o correlación perfecta, mientras más cercano a 0 no existe correlación\n",
    " de ningún tipo. Dependiendo de la aplicación un buen coeficiente de correlación puede\n",
    " ser r2>|0.5|\n",
    "\"\"\"\n",
    "print ( \"ECM: %4.4f\"%(mean_squared_error             (y,y_est) )  )\n",
    "print ( \"Coeficiente de correlación: %4.4f\"%(r2_score(y,y_est) )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo con datos reales\n",
    "La base de datos de diabetes constá de 441 registros de 10 rasgos *data*$\\in\\mathbb{R}^{442\\times10}$ y un rasgo de progresión (*target*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data     = datasets.load_diabetes()\n",
    "diabetes = data['data']\n",
    "targets  = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Los 442 datos se parten en un conjunto de entrenamiento y otro de prueba\n",
    " el primero de 342 muetras y el otro 100.\n",
    " @note: para evitar sesgo se seleccionan aleatoriamente los datos de cada conjunto\n",
    "\"\"\"\n",
    "N   = 100\n",
    "idx = np.arange( diabetes.shape[0] )\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "XTrain = diabetes[ idx[:-N] ]\n",
    "YTrain = targets [ idx[:-N] ]\n",
    "\n",
    "XTest = diabetes[ idx[-N:] ]\n",
    "YTest = targets [ idx[-N:] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Se entrena la regresión lineal y se evalúa su desempeño\n",
    "\"\"\"\n",
    "model = LRM.LinearRegression()\n",
    "model.fit( XTrain,YTrain )\n",
    "\n",
    "L = model.predict( XTest )\n",
    "\n",
    "print ( \"ECM: %4.4f\"%(mean_squared_error             (YTest,L) )  )\n",
    "print ( \"Coeficiente de correlación: %4.4f\"%(r2_score(YTest,L) )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el $R^2$ apenas supera el 0.5, lo cual en aplicaciones médicas no es funcional, aunque indica una leve correlación, pero sin grandes implicaciones diagnósticas.\n",
    "\n",
    "En este caso no es posible graficar la regresión ya que está en $\\mathbb{R}^{11}$, sin embargo podría realizarse con scatter_plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión *Least absolute shrinkage and selection operator* (LASSO)\n",
    "\n",
    "Es una regresión lineal tipo LDA, pero que pondera los pesos $w$ por la norma $L_1$ lo que se traduce en hacer la $j$-ésima componente del vector $w$, $$w[j]:j\\subset\\{1,2,..., n\\},$$ exactamente igual a cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: En este caso se entrena LASSO con el algortimo LARS que converge más rápidamente\n",
    " además se utiliza validación cruzada la optimización del parámetro de *shrinkage*\n",
    "\"\"\"\n",
    "model = LassoLarsCV(max_iter=100000,n_jobs=4,cv=8)\n",
    "model.fit( XTrain,YTrain )\n",
    "\n",
    "L = model.predict( XTest )\n",
    "\n",
    "print ( \"ECM: %4.4f\"%(mean_squared_error             (YTest,L) )  )\n",
    "print ( \"Coeficiente de correlación: %4.4f\"%(r2_score(YTest,L) )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aquí se muestra como se relacionan la Y de prueba y la Y estimada por\n",
    "# el método de regresión lineal\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(YTest,L,'o',color='orange',markersize=8)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método LASSO selecciona nativamente los rasgos que más contribuyen a la regresión haciendo cero aquellas que no lo hacen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Se crean tres variables eta0, eta1, eta2 que se agregan a XTrain para ser\n",
    " fuentes de ruido.\n",
    " @note: Se entrenan dos modelos, uno con regresión lineal convencional y otro con LASSO.\n",
    "\"\"\"\n",
    "\n",
    "eta0 = np.ones(  XTrain.shape[0] )[:,np.newaxis]\n",
    "eta1 = np.array( [0 , 1, 2  ]*114 )[:,np.newaxis]\n",
    "eta2 = np.array( [-1, 1, 0  ]*114 )[:,np.newaxis]\n",
    "\n",
    "np.random.shuffle( eta1 )\n",
    "np.random.shuffle( eta2 )\n",
    "\n",
    "XTrain_mod = np.concatenate( (eta0,XTrain,eta1,eta2),axis=1 )\n",
    "\n",
    "model = LRM.LinearRegression()\n",
    "model.fit( XTrain_mod,YTrain )\n",
    "\n",
    "model2 = LassoLarsCV(max_iter=1000000,n_jobs=4,cv=100)\n",
    "model2.fit( XTrain_mod,YTrain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Como los resultados dependen en cada caso depende del conjunto de entrenamiento\n",
    " y la validación cruzada aquí se muestran los obtenidos en este caso. Se observa que\n",
    " LDA y LASSO detectan perfetamente el vector de unos (componente 0). LASSO además de detectar\n",
    " los vectores de ruido en las componentes 10 y 11, también hace cero tres componentes más, es\n",
    " decir, que no contribuyen a la regresión.\n",
    " @output: >>> \n",
    "LDA,   LASSO\n",
    "0.0000,\t0.0000\n",
    "-76.9286,\t0.0000\n",
    "-228.6376,\t-121.6773\n",
    "486.1239,\t481.9079\n",
    "327.5262,\t245.8395\n",
    "-183.6908,\t0.0000\n",
    "55.2152,\t0.0000\n",
    "-123.8803,\t-177.8817\n",
    "114.9689,\t0.0000\n",
    "566.7349,\t484.7121\n",
    "26.2757,\t0.0000\n",
    "2.4359,\t0.0000\n",
    "-3.4907,\t0.0000\n",
    "\"\"\"\n",
    "\n",
    "print ('LDA,   LASSO')\n",
    "for i,j in zip(model.coef_,model2.coef_):\n",
    "    print ('%4.4f,\\t%4.4f'%(i,j) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: Selección de rasgos (componentes) relevantes detectadas por LASSO\n",
    " @note: Salida para este caso\n",
    " @output: >>> array([2, 3, 4, 7, 9])\n",
    "\"\"\"\n",
    "idx = np.arange( model2.coef_.shape[0] )\n",
    "idx = (model2.coef_!=0).astype('int') * idx\n",
    "idx = idx[idx!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @note: De esta forma XTrain_mod queda únicamente de cinco rasgos, para este caso\n",
    " @output: >>> (342, 5)\n",
    "\"\"\"\n",
    "XTrain_mod[:,idx].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea Rapsberry Pi.\n",
    " - Descargar la base de datos de [eficiencia energética](http://archive.ics.uci.edu/ml/datasets/Energy+efficiency)    \n",
    " - Estimar la regresión lineal para *Heating Load* y para *Cooling Load* por separado\n",
    " - ¿Todos los rasgos son relevantes?\n",
    " \n",
    "**Nota:** No olvide separar sus datos en conjuntos de entrenamiento y prueba"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "conjuntos_de_prueba_2d.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
